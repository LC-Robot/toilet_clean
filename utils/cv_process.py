import cv2
import numpy as np
import torch
from ultralytics import YOLO
from ultralytics.models.sam import Predictor as SAMPredictor

import logging
# ç¦ç”¨ Ultralytics çš„æ—¥å¿—è¾“å‡º
logging.getLogger("ultralytics").setLevel(logging.WARNING)


# def choose_model():
#     """Initialize SAM predictor with proper parameters"""
#     model_weight = 'sam_b.pt'
#     overrides = dict(
#         task='segment',
#         mode='predict',
#         #imgsz=1024,
#         model=model_weight,
#         conf=0.01,
#         save=False
#     )
#     return SAMPredictor(overrides=overrides)

def choose_model():
    """Initialize SAM predictor with proper parameters"""
    model_weight = 'sam_l.pt'
    overrides = dict(
        task='segment',
        mode='predict',
        # imgsz=1024,
        model=model_weight,
        conf=0.25,
        save=False
    )
    return SAMPredictor(overrides=overrides)


def segment_image_with_timing(image_path, output_mask='mask1.png'):
    """
    Enhanced version of segment_image that returns timing information
    image_path: can be either a file path (str) or a numpy array (BGR image).
    output_mask: output mask file name.
    Returns: (mask, timing_dict)
    """
    import time
    
    timing = {}
    
    target_class = "toilet"

    # 1. YOLOæ¨¡å‹åŠ è½½æ—¶é—´
    print("  - åŠ è½½YOLOæ£€æµ‹æ¨¡å‹...")
    yolo_load_start = time.time()
    detections, vis_img, yolo_model = detect_objects(image_path, target_class, return_model=True)
    yolo_load_time = time.time() - yolo_load_start
    timing['yolo_model_load'] = yolo_load_time

    if output_mask is not None:
        cv2.imwrite('detection_visualization.jpg', vis_img)

    # 2. å‡†å¤‡ç»™SAMçš„å›¾åƒ
    if isinstance(image_path, str):
        bgr_img = cv2.imread(image_path)
        if bgr_img is None:
            raise ValueError(f"Failed to read image from path: {image_path}")
        image_rgb = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)
    else:
        image_rgb = cv2.cvtColor(image_path, cv2.COLOR_BGR2RGB)

    # 3. SAMæ¨¡å‹åŠ è½½æ—¶é—´
    print("  - åŠ è½½SAMåˆ†å‰²æ¨¡å‹...")
    sam_load_start = time.time()
    predictor = choose_model()
    predictor.set_image(image_rgb)
    sam_load_time = time.time() - sam_load_start
    timing['sam_model_load'] = sam_load_time
    
    # 4. çº¯è¯†åˆ«åˆ†å‰²æ—¶é—´
    print("  - æ‰§è¡Œåˆ†å‰²æ¨ç†...")
    inference_start = time.time()

    # åˆ¤æ–­æ˜¯å¦æœ‰ç›®æ ‡æ£€æµ‹ç»“æœ
    if detections:
        # è‡ªåŠ¨é€‰æœ€é«˜ç½®ä¿¡åº¦
        best_det = max(detections, key=lambda x: x["conf"])
        results = predictor(bboxes=[best_det["xyxy"]])
        center, mask = process_sam_results(results)
        print(f"    Auto-selected {best_det['cls']} with confidence {best_det['conf']:.2f}")
    else:
        # æ‰‹åŠ¨ç‚¹å‡»
        print("    No detections - click on target object")
        cv2.imshow('Select Object', vis_img)

        # åˆå§‹åŒ–å…¨å±€å˜é‡
        point = []
        clicked = False
        def click_handler(event, x, y, flags, param):
            nonlocal clicked
            if event == cv2.EVENT_LBUTTONDOWN:
                print(f"    Clicked at ({x}, {y})")
                point.extend([x, y])
                clicked = True  # æ ‡è®°ç‚¹å‡»å®Œæˆ
        cv2.setMouseCallback('Select Object', click_handler)
        print("    Waiting for user click...")
        # å¾ªç¯ç­‰å¾…ç‚¹å‡»æˆ–ESCé”®
        while not clicked:
            key = cv2.waitKey(10)  # 10mså»¶è¿Ÿï¼Œå‡å°‘CPUå ç”¨
            if key == 27:  # ESCé”®é€€å‡º
                raise ValueError("User cancelled selection")
        cv2.destroyAllWindows()  # å®‰å…¨å…³é—­çª—å£

        if len(point) == 2:
            results = predictor(points=[point], labels=[1])
            center, mask = process_sam_results(results)
        else:
            raise ValueError("No selection made")

    inference_time = time.time() - inference_start
    timing['pure_inference'] = inference_time

    # 5. ä¿å­˜ mask
    if mask is not None and output_mask is not None:
        cv2.imwrite(output_mask, mask, [cv2.IMWRITE_PNG_BILEVEL, 1])
    elif mask is None:
        print("[WARNING] Could not generate mask")

    return mask, timing

# def set_classes(model, target_class):
#     """Set YOLO-World model to detect specific class"""
#     model.set_classes([target_class])

def detect_objects(image_or_path, target_class=None, return_model=False):
    """
    Detect objects with YOLO-World
    image_or_path: can be a file path (str) or a numpy array (image).
    return_model: if True, returns (detections, vis_img, model) for reuse
    Returns: (list of bboxes in xyxy format, visualization image) or (list of bboxes in xyxy format, visualization image, model)
    """
    model = YOLO("yolov8m-world.pt")
    if target_class:
        model.set_classes([target_class])

    results = model.predict(image_or_path, verbose=False)

    # æ£€æŸ¥æ˜¯å¦æœ‰ç»“æœè¿”å›
    if not results or len(results) == 0:
        # å¦‚æœæ²¡æœ‰ç»“æœï¼Œåˆ›å»ºä¸€ä¸ªç©ºç™½å›¾åƒç”¨äºå¯è§†åŒ–
        if isinstance(image_or_path, str):
            vis_img = cv2.imread(image_or_path)
        else:
            vis_img = image_or_path.copy()
        return [], vis_img

    result = results[0]
    boxes = result.boxes
    vis_img = result.plot()  # Get visualized detection results

    # Extract valid detections
    valid_boxes = []
    for box in boxes:
        if box.conf.item() > 0.10:  # Confidence threshold 0.10
            valid_boxes.append({
                "xyxy": box.xyxy[0].tolist(),
                "conf": box.conf.item(),
                "cls": result.names[int(box.cls.item())]
            })

    if return_model:
        return valid_boxes, vis_img, model
    else:
        return valid_boxes, vis_img


def keep_largest_connected_component(mask):
    """
    ä¿ç•™maskä¸­æœ€å¤§çš„è¿é€šåŒºåŸŸï¼Œå»é™¤ä¸è¿é€šçš„å™ªå£°
    
    Args:
        mask: äºŒå€¼mask (0æˆ–255)
    
    Returns:
        cleaned_mask: åªåŒ…å«æœ€å¤§è¿é€šåŒºåŸŸçš„mask
    """
    # ä½¿ç”¨è¿é€šç»„ä»¶åˆ†æ
    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask, connectivity=8)
    
    if num_labels <= 1:  # åªæœ‰èƒŒæ™¯ï¼Œæ²¡æœ‰å‰æ™¯
        return mask
    
    # statsçš„æ ¼å¼: [x, y, width, height, area]
    # ç¬¬0ä¸ªæ ‡ç­¾æ˜¯èƒŒæ™¯ï¼Œä»ç¬¬1ä¸ªå¼€å§‹æ˜¯å‰æ™¯è¿é€šåŒºåŸŸ
    areas = stats[1:, cv2.CC_STAT_AREA]  # è·å–æ‰€æœ‰å‰æ™¯åŒºåŸŸçš„é¢ç§¯
    
    if len(areas) == 0:
        return mask
    
    # æ‰¾åˆ°æœ€å¤§çš„è¿é€šåŒºåŸŸï¼ˆ+1æ˜¯å› ä¸ºè·³è¿‡äº†èƒŒæ™¯æ ‡ç­¾0ï¼‰
    largest_component_label = np.argmax(areas) + 1
    largest_area = areas[np.argmax(areas)]
    
    # åˆ›å»ºåªåŒ…å«æœ€å¤§è¿é€šåŒºåŸŸçš„mask
    cleaned_mask = np.zeros_like(mask)
    cleaned_mask[labels == largest_component_label] = 255
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    total_components = num_labels - 1  # å‡å»èƒŒæ™¯
    print(f"ğŸ” è¿é€šæ€§åˆ†æ:")
    print(f"   - æ£€æµ‹åˆ° {total_components} ä¸ªè¿é€šåŒºåŸŸ")
    print(f"   - æœ€å¤§åŒºåŸŸé¢ç§¯: {largest_area} åƒç´ ")
    if total_components > 1:
        removed_area = np.sum(areas) - largest_area
        print(f"   - å·²ç§»é™¤ {total_components - 1} ä¸ªå™ªå£°åŒºåŸŸ (æ€»è®¡ {removed_area} åƒç´ )")
    
    return cleaned_mask


def process_sam_results(results):
    """Process SAM results to get mask and center point"""
    if not results or not results[0].masks:
        return None, None

    # Get first mask (assuming single object segmentation)
    mask = results[0].masks.data[0].cpu().numpy()
    mask = (mask > 0).astype(np.uint8) * 255
    
    # è¿‡æ»¤æ‰ä¸è¿é€šçš„å™ªå£°åŒºåŸŸï¼Œåªä¿ç•™æœ€å¤§çš„è¿é€šåŒºåŸŸ
    mask = keep_largest_connected_component(mask)

    # Find contour and center
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    if not contours:
        return None, None

    M = cv2.moments(contours[0])
    if M["m00"] == 0:
        return None, mask

    cx = int(M["m10"] / M["m00"])
    cy = int(M["m01"] / M["m00"])
    return (cx, cy), mask


def segment_image(image_path, output_mask='mask1.png'):
    """
    image_path: can be either a file path (str) or a numpy array (BGR image).
    output_mask: output mask file name.
    """

    target_class = "toilet"

    # 2) åˆæ­¥æ£€æµ‹ - YOLO
    detections, vis_img = detect_objects(image_path, target_class)

    # ä¿å­˜æ£€æµ‹å¯è§†åŒ–ç»“æœï¼ˆä»…åœ¨éœ€è¦ä¿å­˜maskæ—¶æ‰ä¿å­˜å¯è§†åŒ–ï¼‰
    if output_mask is not None:
        cv2.imwrite('detection_visualization.jpg', vis_img)

    # 3) å‡†å¤‡ç»™ SAM çš„å›¾åƒ (RGB æ ¼å¼)
    if isinstance(image_path, str):
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œè¯´æ˜æ˜¯å›¾åƒè·¯å¾„
        bgr_img = cv2.imread(image_path)
        if bgr_img is None:
            raise ValueError(f"Failed to read image from path: {image_path}")
        image_rgb = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)
    else:
        # å¦åˆ™å‡è®¾ image_path å°±æ˜¯ä¸€ä¸ª BGR çš„ numpy æ•°ç»„
        image_rgb = cv2.cvtColor(image_path, cv2.COLOR_BGR2RGB)

    # 4) åˆå§‹åŒ– SAM predictor
    predictor = choose_model()
    predictor.set_image(image_rgb)

    # 5) åˆ¤æ–­æ˜¯å¦æœ‰ç›®æ ‡æ£€æµ‹ç»“æœ
    if detections:
        # è‡ªåŠ¨é€‰æœ€é«˜ç½®ä¿¡åº¦
        best_det = max(detections, key=lambda x: x["conf"])
        results = predictor(bboxes=[best_det["xyxy"]])
        center, mask = process_sam_results(results)
        print(f"Auto-selected {best_det['cls']} with confidence {best_det['conf']:.2f}")
    else:
        # æ‰‹åŠ¨ç‚¹å‡»
        print("No detections - click on target object")
        cv2.imshow('Select Object', vis_img)

        # åˆå§‹åŒ–å…¨å±€å˜é‡
        point = []
        clicked = False
        def click_handler(event, x, y, flags, param):
            nonlocal clicked
            if event == cv2.EVENT_LBUTTONDOWN:
                print(f"Clicked at ({x}, {y})")
                point.extend([x, y])
                clicked = True  # æ ‡è®°ç‚¹å‡»å®Œæˆ
        cv2.setMouseCallback('Select Object', click_handler)
        print("Waiting for user click...")
        # å¾ªç¯ç­‰å¾…ç‚¹å‡»æˆ–ESCé”®
        while not clicked:
            key = cv2.waitKey(10)  # 10mså»¶è¿Ÿï¼Œå‡å°‘CPUå ç”¨
            if key == 27:  # ESCé”®é€€å‡º
                raise ValueError("User cancelled selection")
        cv2.destroyAllWindows()  # å®‰å…¨å…³é—­çª—å£

        if len(point) == 2:
            results = predictor(points=[point], labels=[1])
            center, mask = process_sam_results(results)
        else:
            raise ValueError("No selection made")

    # 6) ä¿å­˜ mask
    if mask is not None and output_mask is not None:
        cv2.imwrite(output_mask, mask, [cv2.IMWRITE_PNG_BILEVEL, 1])
        # print(f"Segmentation saved to {output_mask}")
    elif mask is None:
        print("[WARNING] Could not generate mask")

    return mask


if __name__ == '__main__':
    seg_mask = segment_image('/home/le/clean_ws/realsense_output/capture_20251010_105701_0000.png')
    print("Segmentation result mask shape:", seg_mask.shape if seg_mask is not None else None)
